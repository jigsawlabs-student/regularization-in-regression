{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have performed feature selection through a fairly direct strategy.  We remove, or simulate removing features one by one, and then remove those features where the model's score does not decrease when they are absent.\n",
    "\n",
    "So far, we have done this after fitting the model.  However, if we think about it, we may be able to achieve the same goal by changing the cost function.  We'll do so, by not only choosing a regression model that most closely fits to the data, but also where the model has fewer significant features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Simplicity through Coeficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea behind both ridge and lasso regression is to change the linear regression model's cost function so that we are no longer minimizing the sum of the squared errors, but also the total size of the model's coefficients.  Let's learn about this by thinking about the diabetes dataset as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remember that the diabetes dataset is used to predict the progression of a disease through various features like `age`, `sex`, `bmi` and `blood pressure`.  After training a model, we can get something like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$disease\\_progression = 2*age + 20*is\\_male + 8*blood\\_pressure + -3*bmi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind regularization is to limit the total size of these coefficients.  To calculate the total size, we'll start by using the l2 norm, defined as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{l2 norm} =\\sqrt{\\theta_1^2 + \\theta_2^2 + ... \\theta_n^2}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The l2 norm of the coefficients is also written as $||\\theta||_2$ or simply $||\\theta||$.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the for the model above, we have:\n",
    "\n",
    "$||\\theta||_2 = \\sqrt{2^2 + 20^2 + 8^2 + (-3)^2} = $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.840329667841555"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "coef = np.array([2, 20, 8, -3])\n",
    "np.sqrt(np.sum(coef**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the point is that if we minimize the L2 norm, then the size of individual coeficients will decrease, and this will lead to some features that have less of an impact on our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onto ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the task of ridge regression: to minimize SSE as well as the L2 norm of the model's coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\underset{\\theta}{\\text{arg min }}  J(\\theta) = \\sum_{i=1}^n (y_i - f(x_i))^2 $,subject to $|| \\theta||_2 \\le c$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with some visualizations showing how we can achieve both goals.  \n",
    "\n",
    "1. Minimize SSE\n",
    "\n",
    "The first is to our task of minimizing the sum of the squared errors. Now one way to display this task is with a contour plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./contour-plot-lin-regression.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the axes, the $w_1$ and $w_2$ represent the coefficients of two features.  As we know, as we change the weights of our coefficients, the SSE changes.  That is what the circles represent -- the differing costs as the weights are changed.  So the center of the circle is where we can see the $SSE = 300$.  And the next circle shows the weights where the SSE is 400.\n",
    "\n",
    "So this is our an illustration of our SSE for different weights.  And in regression, we find weights where the cost is minimized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a restriction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's talk about the other restriction.  This is that our coefficients cannot exceed a certain size.  Remember that we are measuring this size as $||w||_2 = \\sqrt{w_1^2 + w_2^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is saying that we want the distance from the origin to our weight vector to be no more than a certain number, $c$.  That's what the beklow graph illustrates.  The further these weights are from the origin, the greater the L2 norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./lagrange-axis.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we think of where the L2 norm is a specific number, say 3.  Then we can see that if we draw the set of points with distance 3 from the center, we just have a circle.  And the same thing for every other constant.\n",
    "\n",
    "So each semicircle in the graph above depicts the set of weights where the L2 norm is a constant value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satisfying both Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with ridge regression, we put the two of these together.  Our goal is to find the minimum sum of squares, given that the L2 norm is less than a specific number.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\underset{\\theta}{\\text{arg min }}  J(\\theta) = \\sum_{i=1}^n (y_i - f(x_i))^2 $,subject to $|| \\theta||_2 \\le c$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually placing these two constraints together looks like the image below.  Now look at the image below, and let's say: \n",
    "\n",
    "* we want to minimize the SSE with the L2 norm no greater than 3.  \n",
    "\n",
    "Where on the graph can we do that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./ridge-regression.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our task is to find the weights that minimize the SSE subject $||\\theta || \\le 3$.  All of the weights where $||\\theta || = 3$ is indicated by the corresponding semicircle.  And to minimze the $SSE$, we wind up on the circle with $SSE = 700$.  Any other value would lie at a point with a larger $SSE$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that with ridge regression, we will no longer be minimizing the SSE errors, but will do so subject to a constraining the coefficients to an L2 norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6'], dtype='object')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load up our data and fit a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4429608706133161"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "cali = load_diabetes()\n",
    "X = pd.DataFrame(cali['data'], columns = cali['feature_names'])\n",
    "y = pd.Series(cali['target'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_transformed = scaler.fit_transform(X)\n",
    "X_transformed_df = pd.DataFrame(X_transformed, columns = X.columns)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed_df, y, random_state = 2)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remember, the idea is to reduce not just SSE but the total size of the model's coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s1    -42.379489\n",
       "sex    -9.232746\n",
       "age    -1.735754\n",
       "s6      2.582949\n",
       "s4      6.965704\n",
       "s3      7.395255\n",
       "bp     16.887496\n",
       "bmi    24.442546\n",
       "s2     28.142755\n",
       "s5     40.279982\n",
       "dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_series = pd.Series(index = X.columns, data = model.coef_).sort_values()\n",
    "coef_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to start, the way that we'll calculate the total magnitude of the coefficients is to square each coefficient and take the square root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5291.277142083067"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(coef_series**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4454456415548589"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha = 3)\n",
    "ridge.fit(X_train, y_train)\n",
    "ridge.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2625.570055949969"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_coef_series = pd.Series(index = X.columns, data = ridge.coef_).sort_values()\n",
    "(ridge_coef_series**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s1    -21.409152\n",
       "sex    -8.990593\n",
       "s3     -1.774973\n",
       "age    -1.610662\n",
       "s6      2.718423\n",
       "s4      4.590996\n",
       "s2     11.502600\n",
       "bp     16.758519\n",
       "bmi    24.675290\n",
       "s5     32.095918\n",
       "dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_coef_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s1    -42.379489\n",
       "sex    -9.232746\n",
       "age    -1.735754\n",
       "s6      2.582949\n",
       "s4      6.965704\n",
       "s3      7.395255\n",
       "bp     16.887496\n",
       "bmi    24.442546\n",
       "s2     28.142755\n",
       "s5     40.279982\n",
       "dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
