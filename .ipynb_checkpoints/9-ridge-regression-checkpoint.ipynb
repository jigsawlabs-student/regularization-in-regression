{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson, we saw that larger amounts of variance is generally associated with larger coefficients.  For example, we saw that our features with larger coefficients contributed to higher amounts of variance in our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./variance-in-coef.png\" width=\"90%\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we saw that we one measurement the total magnitude of our coefficients was the L2 norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$||\\theta||_2 =\\sqrt{\\theta_1^2 + \\theta_2^2 + ... \\theta_n^2} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we'll see in this lesson, the idea behind ridge regression is to use a cost function that fits to the data while minimizing the L2 norm of the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onto ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the task of ridge regression is to minimize SSE as well as the L2 norm of the model's coefficients.  Hopefully this will allow us to fit to the data, while reducing the variance that comes with larger coefficients.  \n",
    "\n",
    "To do so, we use the following cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\underset{\\theta}{\\text{arg min }}  J(\\theta) = \\sum_{i=1}^n (y_i - f(x_i))^2 $,subject to $|| \\theta||_2^2 \\le c$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this is simply expresses our two goals.  The expression on the left is SSE, which allows us to fit to the data.  The expression on the right restricts the L2 norm of our coefficients to specific number, $c$, which will limit the size, and thus the variance of our features' parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some visualizations showing these two componentsm and then we'll see how we can pursue these goals simultaneously.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Minimize SSE\n",
    "\n",
    "The first is to our task of minimizing the sum of the squared errors.  This, as we know, is how we measure how closely our hypothesis function predicts the target values.  And in training we adjust the parameters to reduce the SSE.\n",
    "\n",
    "One way to display this task is with a contour plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./contour-plot-lin-regression.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the axes in the above plot, the $w_1$ and $w_2$ represent the coefficients of two features.  As we know, as we change the weights of our coefficients, the SSE changes.  That is what the circles represent -- the differing costs as the weights are changed.  So the center of the circle is where we can see the $SSE = 300$.  And the next circle shows the weights where the $SSE = 400$.\n",
    "\n",
    "So this is our an illustration of our SSE for different weights.  And in regression, we find weights where the cost is minimized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a restriction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's talk about the other component of our cost function.  This is that our coefficients cannot exceed a certain size.  Remember that we are measuring this size as $||w||_2^2 = \\sqrt{w_1^2 + w_2^2}^2 = w_1^2 + w_2^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is saying that we want the distance from the origin to our weight vector to be no more than a certain number, $c$.  That's what the below graph illustrates.  The further these weights are from the origin, the greater the $\\text{L2}$ norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./lagrange-axis.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we think of where the L2 norm is a specific number, say 3.  Then we can see that if we draw the set of points with distance 3 from the center, we just have a circle.  And the same thing for every other constant.\n",
    "\n",
    "So each semicircle in the graph above depicts the set of weights where the L2 norm is a constant value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satisfying both Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with ridge regression, we put the two of these together.  Our goal is to find the minimum sum of squares, given that the L2 norm squared is less than a specific number.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\underset{\\theta}{\\text{arg min }}  J(\\theta) = \\sum_{i=1}^n (y_i - f(x_i))^2 $,subject to $|| \\theta||_2^2 \\le c$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually placing these two constraints together looks like the image below.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./ridge-regression.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> All of the L2 norm restrictions are indicated by the semicircles near the origin.  And the respective SSE scores are the circles in the center of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now looking at the image above, our task is the following: \n",
    "\n",
    "* we want to minimize the SSE with the L2 norm no greater than 3.  \n",
    "\n",
    "> Where on the graph can we do that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize the $SSE$, we wind up on the circle with $SSE = 700$.  Any other value would lie at a point with a larger $SSE$.  So we can see that with ridge regression, we wind up on the combination of coefficients where the L2 norm intersects with a value of the SSE.  By doing so, we are able to balance both fitting to the data and limiting the variance by limiting the L2 Norm of the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice something else about the solution we found above.  We are no longer finding the parameters that minimize our sum of squared errors.  So the introduction of the constraint restricts our model from fitting to the data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./ridge-regression.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the introduction of this constraint introduces bias into our model.  And remember we do this to reduce variance.  So once again, we see the bias-variance  tradeoff.  As we reduce the variance in our model by constraining the magnitude of the model's parameters, we introduce bias by preventing the model from minimizing the SSE.  In lessons, that follow we'll see how to tune this hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we saw how ridge regression minimize the sum of squared errors subject to constraining the magnitude of the coefficients to reduce variance in the model.  We saw that the parameters that minimize the SSE subject to this constraint occur at the intersection, in the graph below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./ridge-regression.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also saw in reducing variance in our model, the constraint introduces variance.  As we'll see later, we'll tune a hyperparameter to balance bias and variance in our model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
