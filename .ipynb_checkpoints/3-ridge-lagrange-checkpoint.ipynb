{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression Lagrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson, we saw how changing our cost function so that our hypothesis function minimizes the sum of the squared errors subject to a constraint, reduces variance in our model, by reducing the size of the parameters.  We also saw that this approach introduces a bias as our model no longer is minimizing the sum of the squared errors, and thus is prevented from fitting to the training data as it would unrestricted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./ridge-regression.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen how we can combine these two components of our cost function graphically, we'll see how we can also combine these components mathematically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the cost function of our ridge regression model looks like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\underset{\\theta}{\\text{arg min }}  J(\\theta) = \\sum_{i=1}^n (y_i - f(x_i))^2 $,subject to $|| \\theta||_2 \\le c$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update our constraint by squaring each side:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\underset{\\theta}{\\text{arg min }}  J(\\theta) = \\sum_{i=1}^n (y_i - f(x_i))^2 $,subject to $|| \\theta||_2^2 \\le c^2$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that we still are simply satisfying the pythagorean theorem of $a^2 + b^2 = c^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now one way to rewrite this problem is to use lagrange multiplication.  With lagrange multiplication, we multiply our constraint function times a coefficient $\\lambda$.  Doing so with the equation above, we get the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\underset{\\theta}{\\text{arg min }}  J(\\theta) = \\sum_{i=1}^n (y_i - f(x_i))^2  + \\lambda(|| \\theta||_2^2 - c^2)$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And expanding this,  we get the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\underset{\\theta}{\\text{arg min }}  J(\\theta) = \\sum_{i=1}^n (y_i - f(x_i))^2  + \\lambda(\n",
    "\\theta_1^2 + \\theta_2^2 - c^2)$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to find the parameters that minimize the equation above, we'll treat different values for lambda as a hyperparameter.  Notice that when we do so, (say we set $\\lambda = 10$), because $c$ is just a number, like 3, then $-(c^2\\lambda)$ is also a number, here $-90$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing minimization, adding or subtracting our function by a constant, $-c^2\\lambda$ will have no impact on the values $\\theta$ that minimize the function.  Because of this, we can simplify our function by removing $-c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing so, our function now looks like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\underset{\\theta}{\\text{arg min }}  J(\\theta) = \\sum_{i=1}^n (y_i - f(x_i))^2  + \\lambda(\\theta_1^2 + \\theta_2^2)$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Variance Tradeoff - Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our cost function for ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\underset{\\theta}{\\text{arg min }}  J(\\theta) = \\sum_{i=1}^n (y_i - f(x_i))^2  + \\lambda(|| \\theta||_2^2)$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, we can view this as two components.  \n",
    "\n",
    "* $\\sum_{i=1}^n (y_i - f(x_i))^2 $\n",
    ">  This is $\\text{SSE}$. By minimizing $\\text{SSE}$, we train our model to predict target values close to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  $\\lambda(|| \\theta||_2^2)$\n",
    ">  This is our constraint. Our constraint reduces the variance of our model.  It does so by simplifying reducing the magnitude of our hypothesis function's parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that if we treat $\\lambda$ as a hyperparameter, we can work with the bias variance tradeoff all over again.  The larger the value of lambda, the more weight the cost function will give to satisfying the constraint, and thus the lower the variance.  But by doing so, the cost function will give less weight to the sum of squared errors and thus increase the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\underset{\\theta}{\\text{arg min }}  J(\\theta) = \\sum_{i=1}^n (y_i - f(x_i))^2  + \\lambda(|| \\theta||_2^2)$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our task will be to  treat $\\lambda$ as a hyperparameter, which we'll tune by assessing performance on a holdout set.  The larger our value of lambda, the lower the variance.  And we'll need to ensure that our our lambda is not so large that it prevents our model from fitting to the training data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we saw how we can combine our SSE and L2 norm constraint with the lagrangian.  This allowed us to change our cost function from: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\underset{\\theta}{\\text{arg min }}  J(\\theta) = \\sum_{i=1}^n (y_i - f(x_i))^2 $,subject to $|| \\theta||_2^2 \\le c$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\underset{\\theta}{\\text{arg min }}  J(\\theta) = \\sum_{i=1}^n (y_i - f(x_i))^2  + \\lambda(|| \\theta||_2^2)$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In doing so, we can still treat this cost function as having two components, our SSE on the left, and the L2 constraint on the right.  In this equation, we treat lambda as a hyperparameter.  As lambda increases, our cost function weighs the L2 constraint more, thus reducing variance in the model.  And the lower this constraint, the less our model becomes biased, as the less it is constrained from fitting to the data by reducing SSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Khan Academy Contour Constrained Maximization](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/lagrange-multipliers-and-constrained-optimization/v/constrained-optimization-introduction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
