{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression and L2 Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson, we saw how error due to variance occurs in our models.  We saw two symptoms of this overfitting:\n",
    "\n",
    "1. Scores that did not generalize to our holdout sets\n",
    "2. Large amounts of variance in our coefficients\n",
    "\n",
    "Now one technique for reducing variance, is by removing features after fitting the model.  However, if we think about it, we may be able to achieve the same goal by changing the cost function.  We'll do so, by not having a cost function that rewards hypothesis function that not only closely fits to the data, but also where the hypothesis function has fewer significant features, and is thus simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Simplicity through Coeficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea behind both ridge regression is to change the linear regression model's cost function so that we are no longer minimizing the sum of the squared errors, but also the total size of the model's coefficients.  Let's look back to our Airbnb dataset and various coefficients in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./variance-in-coef.png\" width=\"90%\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at coefficients above, the features that are the largest contributors to variance are those features with largest coefficients (either positive or negative).  This is because, the larger the coefficients, the larger the variation from model to model, which will then lead to different predictions based on which model is used.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind regularization is to: \n",
    "* Limit the total size of a model's coefficients, and thus limit the variance,\n",
    "* Yet still train a model that fits to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring our Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ridge regression,  we prefer models where the total magnitude of the coefficients is smaller, and we capture this size with the L2 norm.  This is how we define the L2 Norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{L2 norm} =\\sqrt{\\theta_1^2 + \\theta_2^2 + ... \\theta_n^2}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So to calculate the L2 norm of a model's coefficients we square each coefficient and then take the square root.  \n",
    "> \n",
    "> This is also the Cartesian distance formula, in mathematics.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's calculate the $\\text{L2}$ norm of the following model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\text{price} = 3*\\text{accommodates} + 1.5*\\text{guests_included} -20*\\text{review_is_na} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the for the model above, we calculate the $L2$ norm of the coefficients as the following:\n",
    "\n",
    "$\\text{L2_norm} = \\sqrt{3^2 + 1.5^2 + (-20)^2} = 20.279$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.279299790673246"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "coef = np.array([3, 1.5, -20])\n",
    "np.sqrt(np.sum(coef**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, the L2 norm is often denoted as with double pipes on either side, so we can write:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$||\\theta||_2 =\\sqrt{\\theta_1^2 + \\theta_2^2 + ... \\theta_n^2} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or for the model above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$||\\theta||_2 = 20.279$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onto ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with ridge regression, we'll try to minimize the $\\text{L2}$ norm, which will limit the total size of individual coeficients, and this will lead to a decrease in variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do this, by incorporating the L2 norm directly into our cost function, along with our normal task of finding a model that minimizes the sum of the squared errors.  It looks like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\underset{\\theta}{\\text{arg min }}  J(\\theta) = \\sum_{i=1}^n (y_i - f(x_i))^2 $,subject to $|| \\theta||_2 \\le c$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $c$ is a constant.\n",
    "\n",
    "We'll learn more about how to train a model subject to this constraint in the lessons that follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we learned about the L2 norm and how we can use it to get a sense of variance in our linear regression model.  The idea is that the larger our the coefficients of our model, the larger the variance, as variations in the coefficients will produce variations in the predictions of the model.  We can measure the size of the coefficients through the L2 norm which is: \n",
    "\n",
    "$||\\theta||_2 =\\sqrt{\\theta_1^2 + \\theta_2^2 + ... \\theta_n^2} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we saw that we can embed the L2 norm directly into our cost function by changing our cost function to minimize SSE such that we limit the L2 norm of our parameters.  Doing so, our cost function now looks like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\underset{\\theta}{\\text{arg min }}  J(\\theta) = \\sum_{i=1}^n (y_i - f(x_i))^2 $,subject to $|| \\theta||_2 \\le c$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll explore the effect of this new cost function in the lessons that follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with some visualizations showing how we can achieve both goals.  \n",
    "\n",
    "1. Minimize SSE\n",
    "\n",
    "The first is to our task of minimizing the sum of the squared errors. Now one way to display this task is with a contour plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./contour-plot-lin-regression.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the axes, the $w_1$ and $w_2$ represent the coefficients of two features.  As we know, as we change the weights of our coefficients, the SSE changes.  That is what the circles represent -- the differing costs as the weights are changed.  So the center of the circle is where we can see the $SSE = 300$.  And the next circle shows the weights where the SSE is 400.\n",
    "\n",
    "So this is our an illustration of our SSE for different weights.  And in regression, we find weights where the cost is minimized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a restriction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's talk about the other restriction.  This is that our coefficients cannot exceed a certain size.  Remember that we are measuring this size as $||w||_2 = \\sqrt{w_1^2 + w_2^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is saying that we want the distance from the origin to our weight vector to be no more than a certain number, $c$.  That's what the beklow graph illustrates.  The further these weights are from the origin, the greater the L2 norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./lagrange-axis.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we think of where the L2 norm is a specific number, say 3.  Then we can see that if we draw the set of points with distance 3 from the center, we just have a circle.  And the same thing for every other constant.\n",
    "\n",
    "So each semicircle in the graph above depicts the set of weights where the L2 norm is a constant value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satisfying both Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with ridge regression, we put the two of these together.  Our goal is to find the minimum sum of squares, given that the L2 norm is less than a specific number.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\underset{\\theta}{\\text{arg min }}  J(\\theta) = \\sum_{i=1}^n (y_i - f(x_i))^2 $,subject to $|| \\theta||_2^2 \\le c$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually placing these two constraints together looks like the image below.  Now look at the image below, and let's say: \n",
    "\n",
    "* we want to minimize the SSE with the L2 norm no greater than 3.  \n",
    "\n",
    "Where on the graph can we do that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./ridge-regression.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our task is to find the weights that minimize the SSE subject $||\\theta || \\le 3$.  All of the weights where $||\\theta || = 3$ is indicated by the corresponding semicircle.  And to minimze the $SSE$, we wind up on the circle with $SSE = 700$.  Any other value would lie at a point with a larger $SSE$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that with ridge regression, we will no longer be minimizing the $SSE$ errors, but will do so subject to a constraining the coefficients to an $L2$ norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice something else about the solution we found above.  We are no longer finding the parameters that minimize our sum of squared errors.  So the introduction of the constraint restricts our model from fitting to the data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./ridge-regression.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the introduction of this constraint introduces bias into our model.  And remember we do this to reduce variance.  So once again, we see the bias-variance  tradeoff.  As we reduce the variance in our model by constraining the magnitude of the model's parameters, we introduce bias by preventing the model from minimizing the SSE.  In lessons, that follow we'll see how to tune this hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we saw how ridge regression works to reduce variance in a linear regression function.  It does so by minimizing the SSE subject to parameters that constrained from exceeding a certain magnitude.  In ridge regression, we measure this magnitude through the $L2$ norm.  And the function is minimized where the $SSE$ and $||\\theta||_2$ parameters intersect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
