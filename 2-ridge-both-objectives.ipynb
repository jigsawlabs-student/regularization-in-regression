{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge: Dual Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson, we learned about the L2 norm and how we can use it to get a sense of variance in our linear regression model.  The idea is that the larger our the coefficients of our model, the larger the variance, as variations in the coefficients will produce variations in the predictions of the model.  We can measure the size of the coefficients through the L2 norm which is: \n",
    "\n",
    "$||\\theta||_2 =\\sqrt{\\theta_1^2 + \\theta_2^2 + ... \\theta_n^2} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we saw that we can embed the L2 norm directly into our cost function by changing our cost function to minimize SSE such that we limit the L2 norm of our parameters.  Doing so, our cost function now looks like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\underset{\\theta}{\\text{arg min }}  J(\\theta) = \\sum_{i=1}^n (y_i - f(x_i))^2 $,subject to $|| \\theta||_2 \\le c$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll see what it means to explore both goals simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with some visualizations showing how we can achieve both goals.  \n",
    "\n",
    "1. Minimize SSE\n",
    "\n",
    "The first is our task of minimizing the sum of the squared errors. Now one way to display this task is with a contour plot, where $w_1$ and $w_2$ represent our coefficients, and the circles represent the corresponding SSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./contour-plot-lin-regression.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, as we change the weights of our coefficients, the SSE changes.  That is what the circles represent -- the differing SSE costs as the weights are changed.  So the center of the circle is where we can see the $SSE = 300$.  And the next circle shows the weights where the SSE is 400.\n",
    "\n",
    "So this is an illustration of the SSE for different weights.  And in regression, we find weights where the cost is minimized. So here, we would want to choose weights that place us at the center of our circle, as it appears that would minimize the $SSE$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a restriction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's talk about our restriction.  This is that our coefficients cannot exceed a certain size.  Remember that we are measuring this size as $||w||_2 = \\sqrt{w_1^2 + w_2^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So graphically, this means that we want the distance from the origin to our weight vector to be no more than a certain number, $c$.  That's what the below graph illustrates.  That the further these weights are from the origin, the greater the L2 norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./lagrange-axis.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we think of where the $L2$ norm is a specific number, say 3.  Then we can see that if we draw the set of points with distance 3 from the center, we just have a circle.  And the same thing for every other constant.\n",
    "\n",
    "So each semicircle in the graph above depicts the set of weights where the $L2$ norm is a constant value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satisfying both Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with ridge regression, we put the two of these together.  Our goal is to find the minimum sum of squares, given that the $L2$ norm is less than a specific number.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\underset{\\theta}{\\text{arg min }}  J(\\theta) = \\sum_{i=1}^n (y_i - f(x_i))^2 $,subject to $|| \\theta||_2 \\le c$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, placing these two constraints together looks like the image below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./ridge-regression.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at the image above, and let's say: \n",
    "\n",
    "* we want to minimize the SSE with the $L2$ norm no greater than 3.  \n",
    "\n",
    "> Where on the graph can we do that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our task is to find the weights that minimize the SSE subject $||\\theta ||_2 \\le 3$.  All of the weights where $||\\theta ||_2 = 3$ is indicated by the corresponding semicircle.  And to minimze the $SSE$, we wind up on the circle with $SSE = 700$.  Any other value would lie at a point with a larger $SSE$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that with ridge regression, we will no longer be minimizing the $SSE$ errors, but will do so subject to a constraining the coefficients to an $L2$ norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice something else about the solution we found above.  We are no longer finding the parameters that minimize our sum of squared errors.  So the introduction of the constraint restricts our model from fitting to the data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./ridge-regression.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the introduction of this constraint introduces bias into our model.  And remember we do this to reduce variance.  So once again, we see the bias-variance  tradeoff.  As we reduce the variance in our model by constraining the magnitude of the model's parameters, we introduce bias by preventing the model from minimizing the SSE.  In lessons, that follow we'll see how to tune this hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we saw how ridge regression works to reduce variance in a linear regression function.  It does so by minimizing the SSE subject to parameters that constrained from exceeding a certain magnitude.  In ridge regression, we measure this magnitude through the $L2$ norm.  And the function is minimized where the $SSE$ and $||\\theta||_2$ parameters intersect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
